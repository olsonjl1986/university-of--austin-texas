{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Copy of Untitled1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPiQ5k5Z7Giy6KaKY7MEmYH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/olsonjl1986/university-of--austin-texas/blob/main/project%207%20ANN%20class.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-rg-3MAtwfy"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5pJEPl9t10D"
      },
      "source": [
        "# Project outline\n",
        "\n",
        ">- 1) import libraries\n",
        ">- 2) Define Project\n",
        ">- 3) Import Data, get basic statistics, shape of data, sum of missing vaules, and variable types\n",
        ">- 4) Preprocessing the data for the models\n",
        " - Identify outliers and quantify them columns that contain outliers\n",
        ">- 5) EDA\n",
        ">- 6) Transform data if needed\n",
        ">- 7) Build ANN for classification \n",
        "    - \n",
        "    - \n",
        "    - \n",
        ">- 8) Tune ANN according to ROC-AUC curve\n",
        "    - \n",
        "    - \n",
        "    -  \n",
        ">- 9) compare and contrastS\n",
        ">- 10) Conclusion and business insight. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import libraries needed"
      ],
      "metadata": {
        "id": "5bDzN9jA8c88"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "pK5w1TVvhU5Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==2.0"
      ],
      "metadata": {
        "id": "diqppFffhVga",
        "outputId": "1bb6efc1-c4dc-4cd5-c6d4-b720afdee5b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow==2.0\n",
            "  Downloading tensorflow-2.0.0-cp37-cp37m-manylinux2010_x86_64.whl (86.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 86.3 MB 48 kB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0) (1.1.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0) (1.43.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0) (0.2.0)\n",
            "Collecting keras-applications>=1.0.8\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 6.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0) (1.1.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0) (3.17.3)\n",
            "Collecting tensorboard<2.1.0,>=2.0.0\n",
            "  Downloading tensorboard-2.0.2-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 45.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0) (1.13.3)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0) (0.37.1)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0) (1.15.0)\n",
            "Collecting tensorflow-estimator<2.1.0,>=2.0.0\n",
            "  Downloading tensorflow_estimator-2.0.1-py2.py3-none-any.whl (449 kB)\n",
            "\u001b[K     |████████████████████████████████| 449 kB 60.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0) (0.12.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0) (3.3.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0) (1.19.5)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0) (0.8.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==2.0) (3.1.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (57.4.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (3.3.6)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (1.35.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (0.4.6)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (2.23.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (4.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (3.1.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow==2.0) (1.5.2)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7554 sha256=ba0e1bc100bd39522123b8cdfc9f1d77ee66c8f563a805ce2d5b1ac3c4ee1401\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n",
            "Successfully built gast\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, keras-applications, gast, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.7.0\n",
            "    Uninstalling tensorflow-estimator-2.7.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.7.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.7.0\n",
            "    Uninstalling tensorboard-2.7.0:\n",
            "      Successfully uninstalled tensorboard-2.7.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.4.0\n",
            "    Uninstalling gast-0.4.0:\n",
            "      Successfully uninstalled gast-0.4.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.7.0\n",
            "    Uninstalling tensorflow-2.7.0:\n",
            "      Successfully uninstalled tensorflow-2.7.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-probability 0.15.0 requires gast>=0.3.2, but you have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-2.0.2 tensorflow-2.0.0 tensorflow-estimator-2.0.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "gast",
                  "tensorboard",
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 543
        },
        "id": "HU8J06fPtzGM",
        "outputId": "98fae6ad-8360-4213-feab-ab2e7ec302c0"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_theme()\n",
        "\n",
        "# Removes the limit from the number of displayed columns\n",
        "pd.set_option(\"display.max_columns\", None)\n",
        "# Changes the limit of number of displayed rows to 200\n",
        "pd.set_option(\"display.max_rows\", 200)\n",
        "\n",
        "# import function\n",
        "\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "# import scaling \n",
        "from scipy.stats import zscore\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# neural network libraries\n",
        "import tensorflow as tf\n",
        "#from tensorflow import keras # importing keras library\n",
        "from tensorflow.keras import Sequential  # importing the Sequential Model\n",
        "#from keras.layers import Dense       # importing Dense layer\n",
        "\n",
        "from tensorflow.keras.layers import Dense, InputLayer, Dropout\n",
        "from tensorflow.keras.optimizers import Adam # Optimizer\n",
        "#from tensorflow.keras.optimizers import leakyRelu\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "\n",
        "\n",
        "# import libraries for distance calculations within clusters \n",
        "from scipy.spatial.distance import cdist\n",
        "from scipy.spatial.distance import pdist\n",
        "\n",
        "# to perform kmeans clustering, and computing silhouette score\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# to perform hierarchical clustering, compute cophenetic correlation, and create dendrograms\n",
        "\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage, cophenet\n",
        "\n",
        "# import sklearn.metrics\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
        "from sklearn.metrics import (\n",
        "    f1_score,\n",
        "    accuracy_score,\n",
        "    recall_score,\n",
        "    precision_score,\n",
        "    confusion_matrix,\n",
        "    roc_auc_score,\n",
        "    plot_confusion_matrix,\n",
        ")\n",
        "\n",
        "\n",
        "from google.colab import files\n",
        "import io\n",
        "\n",
        "\n",
        "# to compute distances\n",
        "from scipy.spatial.distance import pdist\n",
        "\n",
        "# to perform hierarchical clustering, compute cophenetic correlation, and create dendrograms\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage, cophenet\n",
        "\n",
        "try:\n",
        "    uploaded\n",
        "except NameError:\n",
        "    uploaded = files.upload()\n",
        "\n",
        "data = pd.read_csv(io.BytesIO(uploaded['bank.csv']))\n",
        "\n",
        "data.shape"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-925bfeab3946>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m#from tensorflow import keras # importing keras library\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m  \u001b[0;31m# importing the Sequential Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;31m#from keras.layers import Dense       # importing Dense layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/api/_v2/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_v2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__internal__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_v2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_v2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapplications\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/api/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0m_print_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/api/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__internal__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapplications\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/api/keras/__internal__/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlegacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0m_print_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/api/keras/__internal__/legacy/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrnn_cell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/api/keras/__internal__/legacy/layers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexperimental\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_spec\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInputSpec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_tf_layers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/api/keras/__internal__/legacy/layers/experimental/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0m_print_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodule_wrapper\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_module_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_module_wrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFModuleWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'module_wrapper' from 'tensorflow.python.util' (/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVGHs1XhzLxg"
      },
      "source": [
        "\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dy-C65_vzNAk"
      },
      "source": [
        "### Data Dict\n",
        "- CustomerId: Unique ID which is assigned to each customer\n",
        "- Surname: Last name of the customer \n",
        "- CreditScore: It defines the credit history of the customer.  \n",
        "- Geography: A customer’s location    \n",
        "- Gender: It defines the Gender of the customer   \n",
        "- Age: Age of the customer     \n",
        "- Tenure: Number of years for which the customer has been with the bank\n",
        "- NumOfProducts: It refers to the number of products that a customer has purchased through the bank.\n",
        "- Balance: Account balance\n",
        "- HasCrCard: It is a categorical variable that decides whether the customer has a credit card or not.\n",
        "- EstimatedSalary: Estimated salary \n",
        "- isActiveMember: It is a categorical variable that decides whether the customer is an active member of the bank or not ( Active member in the sense, using bank products regularly, making transactions, etc )\n",
        "- Exited: It is a categorical variable that decides whether the customer left the bank within six months or not. It can take two values \n",
        "                    0=No ( Customer did not leave the bank )\n",
        "\n",
        "                    1=Yes ( Customer left the bank )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "id": "KXkgmuUreTLy",
        "outputId": "e21d8569-82b7-4f5b-e8ed-adffc02f0968"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-047ed65ff157>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Shape of the data\n",
        "- The data has 10k rows and 14 columns"
      ],
      "metadata": {
        "id": "rB7PlPxHev_A"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWi_r2aZvL4d"
      },
      "source": [
        "data.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCwm0J6ZwV5Z"
      },
      "source": [
        "data.describe().T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "AL5QlhhngICq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Insights \n",
        "\n",
        "- Balance has a slight disparity from mean and median values\n",
        "- HAsCrCard has a slight disparity from the mean and median values"
      ],
      "metadata": {
        "id": "4YCdATVvgJl8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check for sum of null values\n",
        "\n",
        "data.isna().sum()"
      ],
      "metadata": {
        "id": "c692Ov-gfYHD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define cat_ cols and numerical columns for EDA \n"
      ],
      "metadata": {
        "id": "qWyq7YgEfhix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUkdiIn6zCJ5"
      },
      "source": [
        "# drop the  Sl_no and customer key column as this will add no value to the cluster analysis\n",
        "#data = data.drop([\"Sl_No\",\"Customer Key\"], axis = 1)\n",
        "#data.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "GfxH22Ri9stA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# insights for the descriptive stats: Avg Credit card and total visits online appear to have outliers\n",
        "#  - This is due to the fact that the median and mean values are quite different. In credit limit there's \n",
        "# a diff. of approx 18,000 and with total visits_online there is a smaller differnece.  "
      ],
      "metadata": {
        "id": "PBsfeSAs9uGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "vUxBXI6OEXdl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5) - Bivariate Analyis\n",
        "- pairplot\n",
        "- correlation"
      ],
      "metadata": {
        "id": "t-A93HYlEsRe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "f5nmpMuqEmCN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YVCOZ3E0Kx8"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DSG-SHTwbSf"
      },
      "source": [
        "sns.pairplot(data,diag_kind = 'kde', hue = \"Exited\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# correlation for continous variables\n",
        "cat_list = [\"NumOfProducts\",\"HasCrCard\",\"IsActiveMember\",\"Tenure\"]\n",
        "num_col = data.columns.drop(cat_list)\n",
        "num_col"
      ],
      "metadata": {
        "id": "QbzQDFyk-po4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_corr_matrix(corr_matrix):\n",
        "\n",
        "    fig, ax = plt.subplots(figsize= (15,10))\n",
        "    ax = sns.heatmap(corr_matrix, \n",
        "                    annot = True,\n",
        "                    linewidths=0.5,\n",
        "                    fmt= \".2f\",\n",
        "                    cmap= \"YlGnBu\");\n",
        "    bottom, top = ax.get_ylim()\n",
        "    ax.set_ylim(bottom + .5, top - 0.5)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "nO2DhOHTOfaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tmp_data= data.copy()\n",
        "numeric_col =[  'CreditScore', \n",
        "        'Age', 'Balance', 'EstimatedSalary']"
      ],
      "metadata": {
        "id": "RYx8Hsl3jG-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corr_df = pd.DataFrame(data= tmp_data,index=tmp_data.index, columns =numeric_col)\n",
        "corr_df = corr_df.corr()"
      ],
      "metadata": {
        "id": "y9G5l2XpOiFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "make_corr_matrix(corr_df)"
      ],
      "metadata": {
        "id": "EVwNhAG-Og0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "NRzIONRFRDAW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's check the categorical columns \n",
        "\n",
        "cat_corr_df = pd.DataFrame(data= tmp_data,index=tmp_data.index, columns =[\"Geogrpahy\",\"Gender\",\"Tenure\",\"NumOfProducts\",\"HasCrCard\"\n",
        ",\"IsActiveMember\"])\n",
        "cat_corr_df = cat_corr_df.corr()\n",
        "make_corr_matrix(cat_corr_df)"
      ],
      "metadata": {
        "id": "x4nLvg0MQum_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "bB9u60OIRCDQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tmp_data.columns"
      ],
      "metadata": {
        "id": "__omr6eBQ9v-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load bivariate functions\n",
        "\n",
        "# edafunctions\n",
        "def histogram_boxplot(data, feature, figsize=(15, 10), kde=False, bins=None):\n",
        "    \"\"\"\n",
        "    Boxplot and histogram combined\n",
        "\n",
        "    data: dataframe\n",
        "    feature: dataframe column\n",
        "    figsize: size of figure (default (15,10))\n",
        "    kde: whether to show the density curve (default False)\n",
        "    bins: number of bins for histogram (default None)\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    f2, (ax_box2, ax_hist2) = plt.subplots(\n",
        "        nrows=2,  # Number of rows of the subplot grid= 2\n",
        "        sharex=True,  # x-axis will be shared among all subplots\n",
        "        gridspec_kw={\"height_ratios\": (0.25, 0.75)},\n",
        "        figsize=figsize,\n",
        "    )  # creating the 2 subplots\n",
        "    sns.boxplot(\n",
        "        data=data, x=feature, ax=ax_box2, showmeans=True, color=\"violet\"\n",
        "    )  # boxplot will be created and a star will indicate the mean value of the column\n",
        "    sns.histplot(\n",
        "        data=data, x=feature, kde=kde, ax=ax_hist2, bins=bins, palette=\"winter\"\n",
        "    ) if bins else sns.histplot(\n",
        "        data=data, x=feature, kde=kde, ax=ax_hist2\n",
        "    )  # For histogram\n",
        "    ax_hist2.axvline(\n",
        "        data[feature].mean(), color=\"green\", linestyle=\"--\"\n",
        "    )  # Add mean to the histogram\n",
        "    ax_hist2.axvline(\n",
        "        data[feature].median(), color=\"black\", linestyle=\"-\"\n",
        "    )  # Add median to the histogram\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "m5-oBeB9GxGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def distribution_plot_wrt_target(data, predictor, target):\n",
        "    \n",
        "    \"\"\"\n",
        "    box plot and histogram combined\n",
        "\n",
        "    data: dataframe\n",
        "    feature: dataframe column\n",
        "    target: target classification colummn\n",
        "  \n",
        "    \"\"\"\n",
        "    \n",
        "    \n",
        "    import warnings\n",
        "    warnings.filterwarnings(\"ignore\")\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "    sns.set(color_codes=True)  \n",
        "\n",
        "\n",
        "    fig, axs = plt.subplots(2, 2, figsize=(12, 10))\n",
        "\n",
        "    target_uniq = data[target].unique()\n",
        "\n",
        "    axs[0, 0].set_title(\"Distribution of target for target=\" + str(target_uniq[0]))\n",
        "    sns.histplot(\n",
        "        data=data[data[target] == target_uniq[0]],\n",
        "        x=predictor,\n",
        "        kde=True,\n",
        "        ax=axs[0, 0],\n",
        "        color=\"teal\",\n",
        "        stat=\"density\",\n",
        "    )\n",
        "\n",
        "    axs[0, 1].set_title(\"Distribution of target for target=\" + str(target_uniq[1]))\n",
        "    sns.histplot(\n",
        "        data=data[data[target] == target_uniq[1]],\n",
        "        x=predictor,\n",
        "        kde=True,\n",
        "        ax=axs[0, 1],\n",
        "        color=\"orange\",\n",
        "        stat=\"density\",\n",
        "    )\n",
        "\n",
        "    axs[1, 0].set_title(\"Boxplot w.r.t target\")\n",
        "    sns.boxplot(data=data, x=target, y=predictor, ax=axs[1, 0], palette=\"Blues\")\n",
        "\n",
        "    axs[1, 1].set_title(\"Boxplot (with flier removed) w.r.t target\")\n",
        "    sns.boxplot(\n",
        "        data=data,\n",
        "        x=target,\n",
        "        y=predictor,\n",
        "        ax=axs[1, 1],\n",
        "        showfliers=False,\n",
        "        palette=\"Blues\",\n",
        "    )\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "# define column list of tmp_data "
      ],
      "metadata": {
        "id": "BXP_Luc5IZ-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_col"
      ],
      "metadata": {
        "id": "WGx28EmHJS46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_col =[  'CreditScore', \n",
        "        'Age', 'Balance', 'EstimatedSalary']"
      ],
      "metadata": {
        "id": "eldsUJE5Jc-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "for col in numeric_col:\n",
        "  distribution_plot_wrt_target(tmp_data,col,\"Exited\")\n",
        "       \n",
        "        "
      ],
      "metadata": {
        "id": "oBXbxqagH1vL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in numeric_col:\n",
        "  histogram_boxplot(tmp_data, col)"
      ],
      "metadata": {
        "id": "EAsnbhEaJbux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Insights on pairplot.\n",
        "- "
      ],
      "metadata": {
        "id": "9t9d8vdKEDS9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Bivariate Analysis"
      ],
      "metadata": {
        "id": "EXZnlgNHERIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "yOjIl_BRAYQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "chXPFmuhSKJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "A_xMys1YAYvz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# make copies of original dataset to remove and not remove outliers\n",
        "\n",
        "out_data = data.copy()\n",
        "tmp_data = data.copy()"
      ],
      "metadata": {
        "id": "uH2RNjyhcc8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# identify outliers \n",
        "# use IQR to identify outliers. calculate 1st and 2nd quartile, then identify data points that are outside\n",
        "# of the 4 * IQR range\n",
        "balance_quartiles =np.quantile(out_data['Balance'][out_data['Balance'].notnull()], [.25, .75])\n",
        "balance_quartiles_iqr = 4 * (balance_quartiles[1] - balance_quartiles[0])\n",
        "Products_quartiles =np.quantile(out_data['NumOfProducts'][out_data['NumOfProducts'].notnull()], [.25, .75])\n",
        "Products_quartiles_iqr = 4 * (Products_quartiles[1] - Products_quartiles[0])\n",
        "# define the outlier\n",
        "\n",
        "balance_outlier = out_data.loc[np.abs(out_data[\"Balance\"]-out_data[\"Balance\"].median()) > balance_quartiles_iqr, \"Balance\"]\n",
        "product_outlier = out_data.loc[np.abs(out_data[\"NumOfProducts\"]-out_data[\"NumOfProducts\"].median()) > Products_quartiles_iqr, \"NumOfProducts\"]\n",
        "len(balance_outlier)+len(product_outlier)\n"
      ],
      "metadata": {
        "id": "TPcOJ42Ntes8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### outlier treatment \n",
        "- There are no outliers in this dataset"
      ],
      "metadata": {
        "id": "mysL5LSixoZ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tmp_data.shape"
      ],
      "metadata": {
        "id": "vprsk5DvxuIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#cat_list = list(tmp_data.columns)\n",
        "#cat_list.remove(\"Avg_Credit_Limit\")"
      ],
      "metadata": {
        "id": "07Gn504HcV4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5oJuZzwX-mta"
      },
      "source": [
        "for col in cat_list:\n",
        "    \n",
        "    print(100*(data[col].isnull().sum()/len(data[col])))\n",
        "    print(data[col].name)\n",
        "    print(\"-\"*60)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBZ5KARiwxKo"
      },
      "source": [
        "cat_list = [\"NumOfProducts\",\"HasCrCard\",\"IsActiveMember\",\"Tenure\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6KglUmY1jym"
      },
      "source": [
        "# Preprocessing data \n",
        "# - Remove duplicate values\n",
        "\n",
        "print(\"Before dropping of duplciates, data is\",tmp_data.shape[0],\"rows\")\n",
        "tmp_data.drop_duplicates(inplace=True)\n",
        "print(\"after dropping of duplciates, data is\",tmp_data.shape[0],\"rows\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.duplicated().sum()"
      ],
      "metadata": {
        "id": "PgNeB9H0T3it"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "db_b6d9pUScd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get rid of useless rows which are 'RowNumber', 'CustomerId', 'Surname'"
      ],
      "metadata": {
        "id": "X0c9FlXfUD3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tmp_data.drop(['RowNumber', 'CustomerId', 'Surname'], axis = 1, inplace = True)\n"
      ],
      "metadata": {
        "id": "MW8tqvm3UWlV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tmp_data.head(5)"
      ],
      "metadata": {
        "id": "Tfr4kSCKUxuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[\"Gender\"].value_counts()"
      ],
      "metadata": {
        "id": "sYEok5uUXKMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[\"Geography\"].value_counts(0)"
      ],
      "metadata": {
        "id": "YRVHTAx0XqUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_copy = data.copy()"
      ],
      "metadata": {
        "id": "34OPkI09Ymmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use get dummies to encode values \n",
        "data_copy= pd.get_dummies(data_copy, columns = [\"Geography\",\"Gender\"])\n",
        "data_copy.head(5)\n",
        "\n",
        "# drop the row #, Customer ID , and Surname\n",
        "\n",
        "data_copy.drop(columns = [\"RowNumber\",\"CustomerID\", \"Surname\"], axis = 1, inplace = True)\n",
        "data_copy.head()"
      ],
      "metadata": {
        "id": "GSgSJkJeYtri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_copy.drop(columns = [\"RowNumber\",\"CustomerId\", \"Surname\"], axis = 1, inplace = True)\n",
        "data_copy.head()"
      ],
      "metadata": {
        "id": "k_ATq-x5Z6N6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBITrR242SW-"
      },
      "source": [
        "### 3) Univariate analysis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "owfdFmBqU2il"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0TIvWas2kO2"
      },
      "source": [
        "def labeled_barplot(data, feature, perc=False, n=None):\n",
        "    \"\"\"\n",
        "    Barplot with percentage at the top\n",
        "\n",
        "    data: dataframe\n",
        "    feature: dataframe column\n",
        "    perc: whether to display percentages instead of count (default is False)\n",
        "    n: displays the top n category levels (default is None, i.e., display all levels)\n",
        "    \"\"\"\n",
        "    import warnings\n",
        "    warnings.filterwarnings(\"ignore\")\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "    sns.set(color_codes=True)  \n",
        "\n",
        "\n",
        "    total = len(data[feature])  # length of the column\n",
        "    count = data[feature].nunique()\n",
        "    if n is None:\n",
        "        plt.figure(figsize=(count + 2, 6))\n",
        "    else:\n",
        "        plt.figure(figsize=(n + 2, 6))\n",
        "\n",
        "    plt.xticks(rotation=90, fontsize=15)\n",
        "    ax = sns.countplot(\n",
        "        data=data,\n",
        "        x=feature,\n",
        "        palette=\"PuBu\",\n",
        "        order=data[feature].value_counts(ascending=True).index[:n],\n",
        "        hue = None\n",
        "    )\n",
        "\n",
        "    for p in ax.patches:\n",
        "        if perc == True:\n",
        "            label = \"{:.1f}%\".format(\n",
        "                100 * p.get_height() / total\n",
        "            )  # percentage of each class of the category\n",
        "        else:\n",
        "            label = p.get_height()  # count of each level of the category\n",
        "\n",
        "        x = p.get_x() + p.get_width() / 2  # width of the plot\n",
        "        y = p.get_height()  # height of the plot\n",
        "\n",
        "        ax.annotate(\n",
        "            label,\n",
        "            (x, y),\n",
        "            ha=\"center\",\n",
        "            va=\"center\",\n",
        "            size=12,\n",
        "            xytext=(0, 5),\n",
        "            textcoords=\"offset points\",\n",
        "        )  # annotate the percentage\n",
        "\n",
        "    plt.show()  # show the plot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVKAoBiW2l5-"
      },
      "source": [
        "for feature in cat_list:\n",
        "  if feature == \"Total_visits_bank\":\n",
        "    print(\"#\"*50)\n",
        "    print(\"1 and 2 visits attribute to the most of the bank visits\")\n",
        "    print(\"#\"*50)\n",
        "  elif feature == \"Total_Credit_Cards\":\n",
        "    print(\"#\"*50)\n",
        "    print(\"Customers with the 4,6,7 atttribute to the majority of the total credit cards\", \n",
        "    \"within the customers\")\n",
        "    print(\"#\"*50)\n",
        "  elif feature == \"Total_visits_online\":\n",
        "    print(\"#\"*50)\n",
        "    print(\"0-2 visits online attribute to the majority of the online traffic\", \n",
        "    \"The customers don't really like going on the web\")\n",
        "    print(\"#\"*50)\n",
        "  elif feature == \"Total_calls_made\":\n",
        "    print(\"#\"*50)\n",
        "    print(\"approx 70% of the customers fall within 0-4 calls made\")\n",
        "    print(\"#\"*50)\n",
        "  labeled_barplot(tmp_data, feature, perc=True)\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKq--u4623uW"
      },
      "source": [
        "### Define function for stacked barplots\n",
        "\n",
        "def stacked_barplot(data, predictor, target):\n",
        "    \"\"\"\n",
        "    Print the category counts and plot a stacked bar chart\n",
        "\n",
        "    data: dataframe\n",
        "    predictor: independent variable\n",
        "    target: target variable\n",
        "    \"\"\"\n",
        "    count = data[predictor].nunique()\n",
        "    sorter = data[target].value_counts().index[-1]\n",
        "    tab1 = pd.crosstab(data[predictor], data[target], margins=True).sort_values(\n",
        "        by=sorter, ascending=False\n",
        "    )\n",
        "    print(tab1)\n",
        "    print(\"-\" * 120)\n",
        "    tab = pd.crosstab(data[predictor], data[target], normalize=\"index\").sort_values(\n",
        "        by=sorter, ascending=False\n",
        "    )\n",
        "    tab.plot(kind=\"bar\", stacked=True, figsize=(count + 1, 5))\n",
        "    plt.legend(\n",
        "        loc=\"lower left\",\n",
        "        frameon=False,\n",
        "    \n",
        "    )\n",
        "    \n",
        "    plt.legend(loc=\"upper left\", bbox_to_anchor=(1, 1))\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4VNKXZC3IcV"
      },
      "source": [
        "for feature in cat_list:\n",
        "  stacked_barplot(tmp_data, feature,\"NumOfProducts\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Ws1tZx44j6Q"
      },
      "source": [
        "for feature in cat_list:\n",
        "  stacked_barplot(tmp_data, feature,\"HasCrCard\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aU8zgZOj5YVx"
      },
      "source": [
        "for feature in cat_list:\n",
        "  stacked_barplot(tmp_data, feature,\"IsActiveMember\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for feature in cat_list:\n",
        "  stacked_barplot(tmp_data, feature,\"Tenure\")"
      ],
      "metadata": {
        "id": "FIeiexZEmwmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode categorical variaBLES WITH  get dummies\n",
        "\n",
        "\n",
        "#tmp_data = pd.get_dummies(tmp_data,drop_first=True)\n",
        "data_copy = data_copy # wdataset needs to be float to convert to tensors later\n",
        "data_copy.head(5)"
      ],
      "metadata": {
        "id": "xwlESkv5U7ds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "B7JVXC7Oa987"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "db8aVcepz-C0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "NsQQZuhw1hJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data1 = data.copy()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tOIL40YRbTQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data1.drop([\"RowNumber\",\"CustomerId\", \"Surname\"], axis = 1, inplace = True)\n",
        "data1 = pd.get_dummies([\"Geography\",\"Gender\"], data1)"
      ],
      "metadata": {
        "id": "yUMluI611UxN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7) Build ANN mode for classification\n",
        "\n",
        "- Define feature matrices x and y \n",
        "- Define train, test and val \n",
        "- scale data using scaler to standardize all data \n",
        "- Convert data into numpy vectors using .reshape\n",
        "- Instantiate model\n",
        "- Add input layer\n",
        "- add hidden layers \n",
        "- add output layers"
      ],
      "metadata": {
        "id": "V07knCVebz7c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "fldHc2Ga3zCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.dtypes"
      ],
      "metadata": {
        "id": "2v7r74Or5hw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "id": "GYru0fZX5bc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "copy_df = data.copy()"
      ],
      "metadata": {
        "id": "tQgLXLIq6rhc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data1 = pd.DataFrame(data = copy_df, columns= data.columns, index = data.index)\n",
        "data1.head()"
      ],
      "metadata": {
        "id": "FQqaWo1b3p7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data1 = pd.get_dummies(data1)\n"
      ],
      "metadata": {
        "id": "jSSy6c4Q7Tca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define feature matrices X and y \n",
        "\n",
        "X = data1.drop(columns=\"Exited\")\n",
        "Y = data1['Exited']\n",
        "\n",
        "# splitting the data into train, test and val\n",
        "\n",
        "#Splitting the training and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.30, random_state=1)\n",
        "#Splitting the train set into  dev set and training set\n",
        "#X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.20, random_state=1)"
      ],
      "metadata": {
        "id": "FJitEB3fc1xS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "YrnRLUHf7SHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# scale the data\n",
        "from sklearn import preprocessing \n",
        "scaler = preprocessing.StandardScaler()\n",
        "\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "#X_val_scaled = scaler.transform(X_val)\n",
        "\n",
        "X_train_tens = tf.convert_to_tensor(X_train_scaled)\n",
        "X_test_tens = tf.convert_to_tensor(X_test_scaled)\n",
        "\n",
        "\n",
        "\n",
        "y_train_tens = tf.convert_to_tensor(y_train)\n",
        "y_test_tens = tf.convert_to_tensor(y_test)\n",
        "# convert data into numpy vectors\n",
        "#target_scaler = preprocessing.StandardScaler()\n",
        "#y_train = target_scaler.fit_transform(y_train.values.reshape(-1,1)) # Reshape your data using array.reshape(-1, 1) if your data has a single feature\n",
        "#y_test = target_scaler.fit_transform(y_test.values.reshape(-1,1))\n",
        "#y_val=target_scaler.transform(y_val.values.reshape(-1,1))"
      ],
      "metadata": {
        "id": "5yvfbFnWd0Oh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_scaled"
      ],
      "metadata": {
        "id": "p-HeB6bfyw5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert data into tensors\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, InputLayer, Dropout\n",
        "from tensorflow.keras.optimizers import Adam # Optimizer"
      ],
      "metadata": {
        "id": "GjFn8rFYfg5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# build sequential model using tensorflow \n",
        "#from tensorflow import keras\n",
        "#classifier = tf.keras.Sequential()\n",
        "# add input layer \n",
        "#classifier.add(InputLayer(input_shape=X_train_tens[1]))\n",
        "\n",
        "\n",
        "\n",
        "# hidden layer 1 \n",
        "#classifier.add(Dense(10,activation='relu'))\n",
        "#classifier.add(Dense(10, activation ='leakyrelu'))\n",
        "#classifier.add(Dropout(0.5))\n",
        "#classifier.add(Dense(10, activation ='selu'))\n",
        "#classifier.add(Dense(1,activation='sigmoid'))\n",
        "\n",
        "\n",
        "\n",
        "#classifier.compile(optimizer='SGD', loss = \"binary_crossentropy\", metrics=['accuracy'],\n",
        "#batch_size = 64, epochs=64, verbose=1)"
      ],
      "metadata": {
        "id": "nYgHH9WUj88l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eIEO7UN2yPEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential()\n",
        "model.add(InputLayer(input_shape=X_train_tens.shape[1]))\n",
        "\n",
        "model.add(Dense(13, activation = 'relu'))\n",
        "# model.add(Dropout(0.3))\n",
        "model.add(Dense(20, activation = 'relu'))\n",
        "# model.add(Dense(10, activation = 'relu'))\n",
        "model.add(Dropout(0.5)) \n",
        "model.add(Dense(20, activation = 'relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(14, activation = 'relu'))\n",
        "# model.add(Dropout(0.5)) \n",
        "# model.add(Dense(5, activation = 'relu'))\n",
        "\n",
        "# model.add(Dense(20, kernel_initializer='he_normal', activation = 'relu'))\n",
        "# model.add(Dense(20, kernel_initializer='he_normal', activation = 'relu'))\n",
        "# model.add(Dropout(0.5))\n",
        "# model.add(Dense(14, kernel_initializer='he_normal', activation = 'relu'))\n",
        "\n",
        "#model.add(Dense(1, kernel_initializer='he_normal', activation='sigmoid'))\n",
        "\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer=Adam(lr=0.00001), loss = \"binary_crossentropy\", metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(X_train_tens, y_train_tens, validation_split=0.3, batch_size=128, epochs=300, verbose=1)"
      ],
      "metadata": {
        "id": "db6sRDJ3uyZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# modify parameters of neural network to see if accuracy can increase\n",
        "# use leaky relu \n",
        "# add more hidden layers\n",
        "\n",
        "\n",
        "model2 = tf.keras.Sequential()\n",
        "model2.add(InputLayer(input_shape=X_train_tens.shape[1]))\n",
        "\n",
        "model2.add(Dense(13, kernel_initializer='he_normal', activation = 'relu'))\n",
        "model.add(Dropout(.3))\n",
        "model2.add(Dense(20, kernel_initializer='he_normal', activation = 'relu'))\n",
        "# model.add(Dense(10, activation = 'relu'))\n",
        "model2.add(Dropout(0.5)) \n",
        "model2.add(Dense(20,kernel_initializer='he_normal', activation = 'relu'))\n",
        "model2.add(Dropout(0.5))\n",
        "model2.add(Dense(20, activation = 'relu'))\n",
        "# model.add(Dropout(0.5)) \n",
        "# model.add(Dense(5, activation = 'relu'))\n",
        "#model.add(Dropout(0.5))\n",
        "#model.add(Dense(20, kernel_initializer='he_normal', activation = 'relu'))\n",
        "# model.add(Dense(20, kernel_initializer='he_normal', activation = 'relu'))\n",
        "# model.add(Dropout(0.5))\n",
        "# model.add(Dense(14, kernel_initializer='he_normal', activation = 'relu'))\n",
        "\n",
        "#model.add(Dense(1, kernel_initializer='he_normal', activation='sigmoid'))\n",
        "\n",
        "model2.add(Dense(1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
        "\n",
        "model2.compile(optimizer=Adam(lr=0.00001), loss = \"binary_crossentropy\", metrics=['accuracy'])\n",
        "\n",
        "history = model2.fit(X_train_tens, y_train_tens, validation_split=0.3, batch_size=128, epochs=300, verbose=1)\n"
      ],
      "metadata": {
        "id": "Z7-poe8Ptl-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model3 = tf.keras.Sequential()\n",
        "model3.add(InputLayer(input_shape=X_train_tens.shape[1]))\n",
        "\n",
        "model3.add(Dense(13, kernel_initializer='he_normal', activation = 'relu'))\n",
        "model3.add(Dropout(.3))\n",
        "model3.add(Dense(10, kernel_initializer='he_normal', activation = 'relu'))\n",
        "# model.add(Dense(10, activation = 'relu'))\n",
        "model3.add(Dropout(0.5)) \n",
        "model3.add(Dense(9,kernel_initializer='he_normal', activation = 'relu'))\n",
        "#model3.add(Dropout(0.5))\n",
        "model3.add(Dense(8, activation = 'relu'))\n",
        "# model.add(Dropout(0.5)) \n",
        "# model.add(Dense(5, activation = 'relu'))\n",
        "#model.add(Dropout(0.5))\n",
        "#model.add(Dense(20, kernel_initializer='he_normal', activation = 'relu'))\n",
        "# model.add(Dense(20, kernel_initializer='he_normal', activation = 'relu'))\n",
        "# model.add(Dropout(0.5))\n",
        "# model.add(Dense(14, kernel_initializer='he_normal', activation = 'relu'))\n",
        "\n",
        "#model.add(Dense(1, kernel_initializer='he_normal', activation='sigmoid'))\n",
        "\n",
        "model3.add(Dense(1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
        "\n",
        "model3.compile(optimizer=Adam(lr=0.00005), loss = \"binary_crossentropy\", metrics=['accuracy'])\n",
        "\n",
        "history = model3.fit(X_train_tens, y_train_tens, validation_split=0.3, batch_size=256, epochs=300, verbose=1)"
      ],
      "metadata": {
        "id": "wb0udtcaBsHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*italicized text*"
      ],
      "metadata": {
        "id": "lMQgIT5JbyzQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "3TCWwIxCDQSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypertune neural network\n",
        "-use gridsearchcv method to hypertune \n"
      ],
      "metadata": {
        "id": "slwECmdwDQ6o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "def build_model(optimizer):\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(InputLayer(input_shape=X_train_tens.shape[1]))\n",
        "\n",
        "    model.add(Dense(13, kernel_initializer= 'he_normal', activation = 'relu'))\n",
        "\n",
        "    model.add(Dense(20, kernel_initializer= 'he_normal', activation = 'relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(20, kernel_initializer= 'he_normal', activation = 'relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(14, kernel_initializer= 'he_normal', activation = 'relu'))\n",
        "\n",
        "    model.add(Dense(1, kernel_initializer= 'he_normal', activation='sigmoid'))\n",
        "\n",
        "    model.compile(optimizer=optimizer, loss = \"binary_crossentropy\", metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "model = KerasClassifier(build_fn = build_model)\n",
        "parameters = {'batch_size': [32,64,128,256],\n",
        "              'epochs': [25,50,200],\n",
        "              'optimizer': ['adam', 'rmsprop']}\n",
        "\n",
        "     \n",
        "grid_search = GridSearchCV(estimator = model,\n",
        "                           param_grid = parameters,\n",
        "                           scoring = 'recall',\n",
        "                           cv = 5)\n",
        "\n",
        "grid_search = grid_search.fit(X_train_tens, y_train_tens,verbose = 1)\n",
        "best_parameters = grid_search.best_params_\n",
        "best_accuracy = grid_search.best_score_"
      ],
      "metadata": {
        "id": "adHPNlghDZXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### insights\n",
        "\n",
        "- more online visits to the bank imply less visits to the bank \n",
        "- more banks visits transpires to less credit cards owned\n",
        "- as calls increase, so does the amount of visits online increase"
      ],
      "metadata": {
        "id": "CznwJggE-tXG"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUl-_F2b5mcV"
      },
      "source": [
        "for feature in cat_list:\n",
        "  stacked_barplot(tmp_data, feature,\"Total_calls_made\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Insights\n",
        "\n",
        "- As the number of credit cards increase, the total calls made decreases. \n",
        "- the highest number of calls are attributed from customers that have 0 - 2 visits to the bank."
      ],
      "metadata": {
        "id": "wT3YOeVrAMxD"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjjfcSGQ6cxF"
      },
      "source": [
        "\n",
        "def histogram_boxplot(data, feature,hue, figsize=(12, 7), kde=False, bins=None):\n",
        "    \"\"\"\n",
        "    Boxplot and histogram combined\n",
        "\n",
        "    data: dataframe\n",
        "    feature: dataframe column\n",
        "    figsize: size of figure (default (12,7))\n",
        "    kde: show kernel density curve (default False)\n",
        "    bins: number of bins for histogram (default None)\n",
        "    \"\"\"\n",
        "    f2, (ax_box2, ax_hist2) = plt.subplots(\n",
        "        nrows=2,  # Number of rows of the subplot grid= 2\n",
        "        sharex=True,  # x-axis will be shared among all subplots\n",
        "        gridspec_kw={\"height_ratios\": (0.25, 0.75)},\n",
        "        figsize=figsize,\n",
        "    )  # creating the 2 subplots\n",
        "    sns.boxplot(\n",
        "        data=data, x=feature, ax=ax_box2, showmeans=True, color=\"violet\"\n",
        "    )  # boxplot will be created and a star will indicate the mean value of the column\n",
        "    sns.histplot(\n",
        "        data=data, x=feature, kde=kde, ax=ax_hist2,hue=hue, bins=bins, palette=\"winter\"\n",
        "    ) if bins else sns.histplot(\n",
        "        data=data, x=feature, kde=kde, ax=ax_hist2\n",
        "    )  # For histogram\n",
        "    ax_hist2.axvline(\n",
        "        data[feature].mean(), color=\"green\", linestyle=\"--\"\n",
        "    )  # Add mean to the histogram\n",
        "    ax_hist2.axvline(\n",
        "        data[feature].median(), color=\"black\", linestyle=\"-\"\n",
        "    )  # Add median to the histogram"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYZEyx287EFF"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in tmp_data.columns: \n",
        "  histogram_boxplot(tmp_data, i, hue = None)"
      ],
      "metadata": {
        "id": "dX_POtjxuAL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOL1HAqp8tbM"
      },
      "source": [
        "### Process the data \n",
        "# scale the data using zscore scaler by using the apply function\n",
        "\n",
        "tmp_data_Scaled=tmp_data.apply(zscore)\n",
        "# look at the pair plot to identify the different distributions within the data and the scatterplots\n",
        "# from eeach possible relationship\n",
        "sns.pairplot(tmp_data_Scaled,diag_kind='kde')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBC-wDkI_dOa"
      },
      "source": [
        "\n",
        "# look at the correlation between each variable\n",
        "df_corr_scaled = tmp_data_Scaled.corr()\n",
        "df_corr = data.corr()\n",
        "def make_corr_matrix(corr_matrix):\n",
        "\n",
        "    fig, ax = plt.subplots(figsize= (15,10))\n",
        "    ax = sns.heatmap(corr_matrix, \n",
        "                    annot = True,\n",
        "                    linewidths=0.5,\n",
        "                    fmt= \".2f\",\n",
        "                    cmap= \"YlGnBu\");\n",
        "    bottom, top = ax.get_ylim()\n",
        "    ax.set_ylim(bottom + .5, top - 0.5)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "make_corr_matrix(df_corr_scaled)"
      ],
      "metadata": {
        "id": "5wdsSqwsHg43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "make_corr_matrix(df_corr)"
      ],
      "metadata": {
        "id": "Daw1dCwvH5Qj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# insights on correlation matrix: \n",
        "# total visits online vs Avg _credit limit show a positive relation \n",
        "# tota # of credit cards vs Avg_credit limit show a positive relation \n",
        "# - this makes sense for both relations as more visits online, translate to more credit\n",
        "# - more credit card use.\n",
        "# - As more customers visit online the average credit limit increases."
      ],
      "metadata": {
        "id": "cjRjlvRWKZLF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### insights \n",
        "\n",
        "- the average credit limit distribution appears to be right skewed.\n",
        "- given the small amount of data, it is probably best not to remove any samples as we will be faced with overfitting if we get rid of too much of the data.\n",
        "This will give us poor results in real world scenarios deploying the model. "
      ],
      "metadata": {
        "id": "ribqnX6-Bo2C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "S4NFNXEgKYX4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "yPjEj9jIKXBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "CfavHd4ABmjy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "nVtpKc65Bly7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Kmeans Cluster Analyis"
      ],
      "metadata": {
        "id": "RNdYPFkDev8H"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Web1P4IQ82Kj"
      },
      "source": [
        "# Clustering with K-means \n",
        "\n",
        "# Plotting the elbow curve \n",
        "\n",
        "# group data into similar clusters\n",
        "\n",
        "# Use euclidean for distance calculation\n",
        "\n",
        "# define a range for the number of clusters to iterate over\n",
        "clusters = range(1,7)\n",
        "# define an empty list for the mean distortions that will get calculated below \n",
        "euc_meanDistortions =[]\n",
        "\n",
        "# iterate through the range for the number of clusters\n",
        "for k in clusters: \n",
        "\n",
        "  # define the model\n",
        "    euc_model = KMeans(n_clusters=k)\n",
        "\n",
        "    euc_model.fit(tmp_data_Scaled)\n",
        "  # fit the model on the scaled data \n",
        "    prediction=euc_model.predict(tmp_data_Scaled)\n",
        "  # make predictions on the tmp scaled data after it has been fit\n",
        "  euc_meanDistortions.append(sum(np.min(cdist(tmp_data_Scaled,euc_model.cluster_centers_,\"euclidean\"),axis=1))/tmp_data_Scaled.shape[0])\n",
        "    # use the two arrays tmp_data_scaled and euc_model.cluster_centers_ to calculate the distance between each pair of values\n",
        "    # divide that by the number of rows in the tmp_data_Scaled df\n",
        "    # then take the sum of the minumum distance computed then append that in the mean_dist list\n",
        "    \n",
        "plt.plot(clusters, euc_meanDistortions,\"bx-\")\n",
        "plt.xlabel('K')\n",
        "plt.ylabel(\"Average Dist\")\n",
        "plt.title(\"selecting k with the elbow method\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "euc_model.labels_"
      ],
      "metadata": {
        "id": "pLcMiXZ5O5rw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "euc_model.cluster_centers_"
      ],
      "metadata": {
        "id": "g7iuJNKCS-On"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "euc_meanDistortions"
      ],
      "metadata": {
        "id": "_ytYKoSVTk_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clustering with K-means \n",
        "\n",
        "# Plotting the elbow curve \n",
        "\n",
        "# group data into similar clusters\n",
        "\n",
        "# use \"minkowski\" for ditance calculation\n",
        "\n",
        "clusters = range(1,7)\n",
        "mink_meanDistortions =[]\n",
        "\n",
        "for k in clusters: \n",
        "    mink_model = KMeans(n_clusters=k)\n",
        "    mink_model.fit(tmp_data_Scaled)\n",
        "    prediction=mink_model.predict(tmp_data_Scaled)\n",
        "    mink_meanDistortions.append(sum(np.min(cdist(tmp_data_Scaled,mink_model.cluster_centers_,\"minkowski\", p = 4),axis=1))/tmp_data_Scaled.shape[0])\n",
        "    \n",
        "    \n",
        "plt.plot(clusters, mink_meanDistortions,\"bx-\")\n",
        "plt.xlabel('K')\n",
        "plt.ylabel(\"Average Dist\")\n",
        "plt.title(\"selecting k with the elbow method\")"
      ],
      "metadata": {
        "id": "noBysGv7CcDk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clustering with K-means \n",
        "\n",
        "# Plotting the elbow curve \n",
        "\n",
        "# group data into similar clusters\n",
        "\n",
        "# use \"city block\" for ditance calculation\n",
        "\n",
        "clusters = range(1,7)\n",
        "city_meanDistortions =[]\n",
        "\n",
        "for k in clusters: \n",
        "    model = KMeans(n_clusters=k)\n",
        "    model.fit(tmp_data_Scaled)\n",
        "    prediction=model.predict(tmp_data_Scaled)\n",
        "    city_meanDistortions.append(sum(np.min(cdist(tmp_data_Scaled,model.cluster_centers_,\"cityblock\"),axis=1))/tmp_data_Scaled.shape[0])\n",
        "    \n",
        "    \n",
        "plt.plot(clusters, city_meanDistortions,\"bx-\")\n",
        "plt.xlabel('K')\n",
        "plt.ylabel(\"Average Dist\")\n",
        "plt.title(\"selecting k with the elbow method\")"
      ],
      "metadata": {
        "id": "aTHnlCnAC1qn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clustering with K-means \n",
        "\n",
        "# Plotting the elbow curve \n",
        "\n",
        "# group data into similar clusters\n",
        "\n",
        "# use \"chebyshev\" for distance calculation\n",
        "\n",
        "clusters = range(1,7)\n",
        "cheby_meanDistortions =[]\n",
        "\n",
        "for k in clusters: \n",
        "    cheby_model = KMeans(n_clusters=k)\n",
        "    cheby_model.fit(tmp_data_Scaled)\n",
        "    prediction=cheby_model.predict(tmp_data_Scaled)\n",
        "    cheby_meanDistortions.append(sum(np.min(cdist(tmp_data_Scaled,cheby_model.cluster_centers_,\"chebyshev\"),axis=1))/tmp_data_Scaled.shape[0])\n",
        "    \n",
        "    \n",
        "plt.plot(clusters, cheby_meanDistortions,\"bx-\")\n",
        "plt.xlabel('K')\n",
        "plt.ylabel(\"Average Dist\")\n",
        "plt.title(\"selecting k with the elbow method\")"
      ],
      "metadata": {
        "id": "VHeZayE4DQl3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "7zewL0r7CW6E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot all of the distance methods together on one graph to compare each method"
      ],
      "metadata": {
        "id": "OW_C6TKjUand"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "plt.plot(clusters,euc_meanDistortions,\"bx-\", label = \"Euclidean\", color= \"blue\")\n",
        "plt.plot(clusters,city_meanDistortions,\"bx-\", label = \"city\", color = \"red\")\n",
        "plt.plot(clusters,mink_meanDistortions,\"bx-\", label = \"Minkowski\", color = \"brown\")\n",
        "plt.plot(clusters,cheby_meanDistortions,\"bx-\", label = \"chebyshev\",color = \"orange\")\n",
        "plt.xlabel('K')\n",
        "plt.ylabel(\"Average Dist\")\n",
        "plt.legend()\n",
        "plt.title(\"selecting k with the elbow method\")\n",
        "plt.figure(figsize= (1,1))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9Jab1CBmUtID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### insights on comparing of the different elbow methods using the differnt distances\n",
        "# - All 3 of the minkowkski, euclidean, and chebyshev are relatively similar.\n",
        "#    - Cityblock method had the highest amount of distance between clusters and using this shows the greatest distance between when k = 1\n",
        "#    to k = 3, therefore using this ultimately to decide the appropriate k for further cluster profiling."
      ],
      "metadata": {
        "id": "DeJQX8U4XWye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### insights on the elbow method\n",
        "- Through each method of plotting the elbow method using the 4 different distances(euclidean, minkowski, cityclock , and chebyshev. \n",
        "    - Elbow method depicts each time as k = 3 for optimal solution.\n",
        "    - With the different the methods the ditance between each cluster changes. \n",
        "    - Highest distance occurs with the cityblock method."
      ],
      "metadata": {
        "id": "TI7H9ePLJOKG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "EJG9_as-OXdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "uuHRYNsiUYj_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fitting K-means model\n",
        "\n",
        "km_start = time.time()\n",
        "kmeans = KMeans(n_clusters=3,random_state=1)\n",
        "kmeans.fit(tmp_data_Scaled)\n",
        "km_end= time.time()\n",
        "\n",
        "km_run_time = km_end - km_start\n",
        "print(\"the total km run time is\",km_run_time)\n",
        " \n",
        "\n",
        "# adding kmeans cluster labels to the original and scaled dataframes\n",
        "tmp_data_Scaled[\"KM_segments\"] = kmeans.labels_\n",
        "\n",
        "#df2 = df.copy()\n",
        "tmp_data[\"KM_segments\"] = kmeans.labels_\n",
        "\n",
        " \n",
        "\n",
        "# cluster profiling\n",
        "\n",
        "km_cluster_profile = tmp_data.groupby(\"KM_segments\").mean()\n",
        "\n",
        "km_cluster_profile[\"count_in_each_segment\"] = (\n",
        "tmp_data.groupby(\"KM_segments\")[\"Total_Credit_Cards\"].count().values\n",
        ")\n",
        "\n",
        "km_cluster_profile.style.highlight_max(color=\"lightgreen\", axis=0)"
      ],
      "metadata": {
        "id": "9UslAlxoN4lx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cob_edLV_sbh"
      },
      "source": [
        "# the elbow occurs when k = 3 \n",
        "# profile the clusters when k = 3 \n",
        "\n",
        "# make empty list for the silhoette scores to be placed in\n",
        "sil_score = []\n",
        "# define a range of number of clusters to try\n",
        "cluster_list = range(2, 8)\n",
        "\n",
        "for n_clusters in cluster_list:\n",
        "  #instantiate Kmeans cluster model \n",
        "  clusterer = KMeans(n_clusters=n_clusters, random_state=1)\n",
        "  # make predictions and fit with the tmp_data_scaled data frame\n",
        "  preds = clusterer.fit_predict((tmp_data_Scaled))\n",
        "  \n",
        "  # define the silhouette score funtion and pass is the scaled data frame and the mode \n",
        "  # predictions\n",
        "  score = silhouette_score(tmp_data_Scaled, preds)\n",
        "\n",
        "  # append the silhouette score to the empty list \n",
        "  sil_score.append(score)\n",
        "  \n",
        "  # print the individual siilhouette score for each value of k\n",
        "  print(\"For n_clusters = {}, the silhouette score is {})\".format(n_clusters, score))\n",
        "\n",
        "plt.plot(cluster_list, sil_score)\n",
        "\n",
        "plt.xlabel('K')\n",
        "plt.ylabel(\"silhouette score\")\n",
        "\n",
        "plt.title(\"silhouette score vs k value\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tmp_data_Scaled.boxplot(by='KM_segments', layout = (2,4),figsize=(15,10))"
      ],
      "metadata": {
        "id": "qGDuYlSUZouQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FcYFUBzPDpnJ"
      },
      "source": [
        "### Insights with cluster profiling for k means clustering\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hierarchial Clustering"
      ],
      "metadata": {
        "id": "9D7zRFrnQ-4G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "L6qu8gVrXyAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### plot dendograms and determine what best amount clusters to use\n",
        "- Instantiate Clustering algorithm\n",
        "- Fit Model\n",
        "- plot dendogram and determine best # of clusters based upon cophentic corr"
      ],
      "metadata": {
        "id": "c0Wla0b_X1cU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dist_metrics = [\"euclidean\", \"chebyshev\", \"mahalanobis\", \"cityblock\"]\n",
        "linkage_methods = [\"single\", \"complete\", \"average\", \"weighted\"]\n",
        "\n",
        "\n",
        "# determine method and linkage that gives you max value of cop. correlation\n",
        "\n",
        "# use two for loops to iterate through the distance metrics and the \n",
        "# linkage methods\n",
        "\n",
        "for metrics in dist_metrics:\n",
        "  for link in linkage_methods:\n",
        "    # instantiate linkage object\n",
        "    #calculate a linkage for each metric and linkage method \n",
        "    Z= linkage(tmp_data_Scaled, metric= metrics, method = link )\n",
        "    # use the linkage Z to calculate the cophenetic correlation\n",
        "    C, cophentic_distance = cophenet(Z, pdist(tmp_data_Scaled))\n",
        "    # print each distance and linkage correlation value\n",
        "    print(\"the cophenetic corr for distance\",metrics, \"and linkage\", link, C )\n"
      ],
      "metadata": {
        "id": "_bNtiCjvY8FK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the highest cophenetic correlation uses euclidean as distance and average as linkage.\n",
        "\n",
        "# Explore different linkage methods using euclidean distances and plot dendograms of each.\n",
        "\n",
        "# list of linkage methods\n",
        "linkage_methods = [\"single\", \"complete\", \"average\", \"centroid\", \"ward\", \"weighted\"]\n",
        "\n",
        "# lists to save results of cophenetic correlation calculation\n",
        "\n",
        "compare = []\n",
        "\n",
        "# to create a subplot image\n",
        "fig, axs = plt.subplots(len(linkage_methods), 1, figsize=(15, 30))\n",
        "\n",
        "# We will enumerate through the list of linkage methods above\n",
        "# For each linkage method, we will plot the dendrogram and calculate the cophenetic correlation\n",
        "for i, method in enumerate(linkage_methods):\n",
        "    # calulate the linkage Z for the metric euclidean along with each linkage method in the linkage_method list\n",
        "    Z = linkage(tmp_data_Scaled, metric=\"euclidean\", method=method)\n",
        "\n",
        "    # create a dendogram with each index value in linkage method\n",
        "    dendrogram(Z, ax=axs[i])\n",
        "    # set title for each dendogram with each separate linkage method in parantheses \n",
        "    axs[i].set_title(f\"Dendrogram ({method.capitalize()} Linkage)\")\n",
        "\n",
        "    coph_corr, coph_dist = cophenet(Z, pdist(tmp_data_Scaled))\n",
        "    axs[i].annotate(\n",
        "        f\"Cophenetic\\nCorrelation\\n{coph_corr:0.2f}\",\n",
        "        (0.80, 0.80),\n",
        "        xycoords=\"axes fraction\",\n",
        "    )\n",
        "\n",
        "    compare.append([method, coph_corr])"
      ],
      "metadata": {
        "id": "3WJzOdEzd7RP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "IaIyskVIX0xP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create dataframe to compare columns\n",
        "compare_cols = [\"Linkage\", \"Cophenetic Coefficient\"]\n",
        "# make dataframe from cophenetic corr and compare columns\n",
        "compare_df = pd.DataFrame(compare, columns = compare_cols)\n",
        "compare_df"
      ],
      "metadata": {
        "id": "qt7ZiKuXgkus"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max(compare_df[\"Cophenetic Coefficient\"])"
      ],
      "metadata": {
        "id": "HsSTXxzDg0Hu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "SAy2eZnjhCA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Insights on hierarchial clustering\n",
        "- THe optimal number of clusters to use is 3 \n",
        "- The dendogram with average linkage, results in a .91 Coph. Corr\n",
        "  - the avg linkage method showed good separation in height between each cluster, this is why it has the highest coph. corr.\n",
        "- The second highest is with the centroid linkage."
      ],
      "metadata": {
        "id": "6O2hUAjRh_Be"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Cluster profiling with euclidean and average distance with k = 3\n"
      ],
      "metadata": {
        "id": "AA41_4JRies8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# calulate time to perform calc\n",
        "import time\n",
        "hc_start = time.time()\n",
        "hieracrchial_cluster = AgglomerativeClustering(n_clusters=3, affinity=\"euclidean\", linkage=\"average\")\n",
        "hieracrchial_cluster.fit(tmp_data_Scaled)\n",
        "hc_end = time.time()\n",
        "hc_run_time = hc_end - hc_start\n",
        "# add labels to the clusters to the scaled data frame\n",
        "\n",
        "tmp_data_Scaled[\"cluster_labels\"] = hieracrchial_cluster.labels_\n",
        "tmp_data[\"cluster_labels\"] = hieracrchial_cluster.labels_\n",
        "\n",
        "print(\"the total run time for hc is\", hc_run_time)"
      ],
      "metadata": {
        "id": "FhlU6EFWil2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cluster profiling\n",
        "\n",
        "hc_cluster_profile = tmp_data.groupby(\"cluster_labels\").mean()\n",
        "\n",
        "hc_cluster_profile[\"count_in_each_segment\"] = (\n",
        "tmp_data.groupby(\"cluster_labels\")[\"Avg_Credit_Limit\"].count().values\n",
        ")\n",
        "\n",
        "hc_cluster_profile.style.highlight_max(color=\"lightgreen\", axis=0)"
      ],
      "metadata": {
        "id": "OrNZyGEfi01m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hc_cluster_profile.drop(\"KM_segments\", axis = 1, inplace= True)"
      ],
      "metadata": {
        "id": "sP5Rs5HWlGnN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hc_cluster_profile.style.highlight_max(color=\"lightgreen\", axis=0)"
      ],
      "metadata": {
        "id": "0qpU1L_ilCSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "km_cluster_profile.style.highlight_max(color=\"lightgreen\", axis=0)"
      ],
      "metadata": {
        "id": "G1ixLYrKjpV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tmp_data_Scaled.boxplot(by='cluster_labels', layout = (2,4),figsize=(15,10))"
      ],
      "metadata": {
        "id": "L8n_TOHIlg0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tmp_data_Scaled.boxplot(by='KM_segments', layout = (2,4),figsize=(15,10))"
      ],
      "metadata": {
        "id": "JFNk5rBhmZG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "fig, axes = plt.subplots(1, 5, figsize=(16, 6))\n",
        "fig.suptitle(\"Boxplot of scaled numerical variables for each cluster HC\", fontsize=20)\n",
        "counter = 0\n",
        "for i in range(5):\n",
        "    sns.boxplot(\n",
        "        ax=axes[i],\n",
        "        y=tmp_data_Scaled[tmp_data_Scaled.columns[counter]],\n",
        "        x=tmp_data_Scaled[\"cluster_labels\"]\n",
        "    )\n",
        "    counter = counter + 1\n",
        "\n",
        "fig.tight_layout(pad=2.0)"
      ],
      "metadata": {
        "id": "6mE5V4NYoSWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "fig, axes = plt.subplots(1, 5, figsize=(16, 6))\n",
        "fig.suptitle(\"Boxplot of scaled numerical variables for each cluster Kmeans\", fontsize=20)\n",
        "counter = 0\n",
        "for i in range(5):\n",
        "    sns.boxplot(\n",
        "        ax=axes[i],\n",
        "        y=tmp_data_Scaled[tmp_data_Scaled.columns[counter]],\n",
        "        x=tmp_data_Scaled[\"KM_segments\"]\n",
        "    )\n",
        "    counter = counter + 1\n",
        "\n",
        "fig.tight_layout(pad=2.0)"
      ],
      "metadata": {
        "id": "HzwMGpWGpWPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_z8bZFvkrrjn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### comparison of both cluster methods Kmeans and Hierarchial\n",
        "- Kmeans and Hierarchial both gave the same amount of clusers to use.\n",
        "With k = 3 with both methods gave the highest performance\n",
        "- Using kmeans takes substantially more computing resources as it almost doubles in run time. It takes 155% longer to run the kmeans algorithm\n",
        " - so if this model was to deploy and it had 2million rows of data the \n",
        " savings would be substantial. \n",
        "- Upon examining these with boxplots between HC and Kmeans, all the data is the same just the cluster identification is different. \n"
      ],
      "metadata": {
        "id": "MXWxrkRRnSsg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "WN5Y9jA-nRUP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"the total km run time is\",km_run_time)\n",
        "print(\"the total run time for hc is\", hc_run_time)"
      ],
      "metadata": {
        "id": "ULCjnsf5pYeY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "km_run_time/hc_run_time"
      ],
      "metadata": {
        "id": "hlpzGisEpfKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "bVFdZf90p3Vg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conclusions and business reccomendations \n",
        "\n",
        "-  I would reccomend to use Hierarchical clustering for real world model deployment. This took substanially less time for the model to run, therefore \n",
        "translating to less cost for the business to manage.  \n",
        "\n",
        "- Customers with a moderate amount of total credit cards, 4,6,7 contribute approx. 54% of the customer base,\n",
        "  - While the customers with 8,9,10 credit cards contribute to the lowest amount of customer base. Customers with 1,2,3,5 credit card contribute to the 2nd highest contributing group.\n",
        "  - 0-2 visits online contribute for about 67% of the customer base. 0-4 calls made attribute to approx. 71% of the customers. \n",
        "- Customers with the highest credit limits have a higher amount of credit cards and spend more time using the online services. \n",
        "- Customers with moderate credit limits have approx.  6 credit cards in average, primarily visit the bank and are the highest of the market segmentation at 58% of the customers. \n",
        "- The low level credit limit customers have few credit cards, approx. 3 on average, primarily handle their business using phone services and they make up 33% of the customers. \n",
        "\n",
        "The marketing team to focus on reaching out to this group of moderate and low level credit limit customers, as they will more than likely gain more revenue in fees from the lower and moderate credit limit customers and they make up 91% of the customer base. Work with the marketing team to get higher turnover with customers using the online services as well. This will help the highest customer segment reduce demand in the branches of the bank and reduce manpower needed for brick and mortar banks.  \n"
      ],
      "metadata": {
        "id": "5d3Df_2Ytqx4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "yPUwdZYy8Dl3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}